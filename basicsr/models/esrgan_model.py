import torch
from collections import OrderedDict

from basicsr.utils.registry import MODEL_REGISTRY
from .srgan_model import SRGANModel

from collections import OrderedDict
import torch
import torch.nn as nn
import torch.nn.functional as F

import pywt  # Wavelet Transform ë¼ì´ë¸ŒëŸ¬ë¦¬
import numpy as np
import skimage.feature

# Self-Similarity Loss (ìžê¸° ìœ ì‚¬ì„± ìœ ì§€)
def patch_similarity_loss(pred, target, patch_size=8):
    B, C, H, W = pred.shape
    loss = 0.0
    count = 0

    for i in range(0, H - patch_size + 1, patch_size):
        for j in range(0, W - patch_size + 1, patch_size):
            pred_patch = pred[:, :, i:i+patch_size, j:j+patch_size]
            target_patch = target[:, :, i:i+patch_size, j:j+patch_size]

            if pred_patch.shape[2:] == (patch_size, patch_size):
                # **ì±„ë„(C) ì°¨ì›ê¹Œì§€ í¬í•¨í•˜ì—¬ ìœ ì‚¬ë„ë¥¼ ë¹„êµ**
                sim = F.cosine_similarity(pred_patch.permute(0, 2, 3, 1).flatten(1),
                                          target_patch.permute(0, 2, 3, 1).flatten(1), dim=1)
                l1_diff = F.l1_loss(pred_patch, target_patch)

                loss += torch.mean(1 - sim) + 0.1 * l1_diff  # ìœ ì‚¬ì„± + í”½ì…€ ì°¨ì´ ê³ ë ¤
                count += 1

    return loss / count if count > 0 else loss

# WaveletHighFrequencyLoss í´ëž˜ìŠ¤ - pywt import ì˜¤ë¥˜ ìˆ˜ì •
class WaveletHighFrequencyLoss(nn.Module):
    def __init__(self):
        super(WaveletHighFrequencyLoss, self).__init__()

        # Haar Wavelet í•„í„° (Low-Pass, High-Pass)
        self.haar_filter = torch.tensor([[1, 1], [1, 1]], dtype=torch.float32).view(1, 1, 2, 2)
        self.haar_high_filter = torch.tensor([[1, -1], [-1, 1]], dtype=torch.float32).view(1, 1, 2, 2)

    def wavelet_transform(self, img):
        """
        Haar Wavelet Transformì„ ì‚¬ìš©í•˜ì—¬ 1ì±„ë„ ì´ë¯¸ì§€ì˜ ê³ ì£¼íŒŒ ì„±ë¶„ì„ ë¶„ë¦¬.
        """
        img = F.pad(img, (1, 1, 1, 1), mode='reflect')  # ê°€ìž¥ìžë¦¬ ë°˜ì‚¬ íŒ¨ë”© ì¶”ê°€
        low_freq = F.conv2d(img, self.haar_filter.to(img.device), stride=2)
        high_freq = F.conv2d(img, self.haar_high_filter.to(img.device), stride=2)

        return torch.abs(high_freq)  # ê³ ì£¼íŒŒ ì„±ë¶„ ë°˜í™˜

    def sobel_filter(self, img):
        """
        Sobel Filterë¥¼ ì‚¬ìš©í•˜ì—¬ 1ì±„ë„ ì´ë¯¸ì§€ì˜ ê²½ê³„ì„  ì •ë³´ë¥¼ ì¶”ì¶œ.
        """
        sobel_x = torch.tensor([[1, 0, -1], [2, 0, -2], [1, 0, -1]], dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(img.device)
        sobel_y = torch.tensor([[1, 2, 1], [0, 0, 0], [-1, -2, -1]], dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(img.device)

        grad_x = F.conv2d(img, sobel_x, padding=1)
        grad_y = F.conv2d(img, sobel_y, padding=1)

        return torch.sqrt(grad_x**2 + grad_y**2 + 1e-6)  # Sobel Edge Magnitude

    def forward(self, pred, target):
        """
        ì˜ˆì¸¡ëœ ì´ë¯¸ì§€ì™€ ì‹¤ì œ ì´ë¯¸ì§€ì˜ ê³ ì£¼íŒŒ ì„±ë¶„ì„ ë¹„êµí•˜ëŠ” Loss ê³„ì‚°.
        """
        pred_wavelet = self.wavelet_transform(pred)
        target_wavelet = self.wavelet_transform(target)

        pred_sobel = self.sobel_filter(pred)
        target_sobel = self.sobel_filter(target)

        # L1 Lossë¡œ ê³ ì£¼íŒŒ ì„±ë¶„ ì°¨ì´ë¥¼ ë¹„êµ
        loss_wavelet = F.l1_loss(pred_wavelet, target_wavelet)
        loss_sobel = F.l1_loss(pred_sobel, target_sobel) * 0.5  # Sobel ê°€ì¤‘ì¹˜ ì¡°ì ˆ

        return loss_wavelet + loss_sobel  # ë‘ ê°€ì§€ ê³ ì£¼íŒŒ ì†ì‹¤ì„ í•©ì‚°

# CharbonnierLoss (RMSE ìµœì í™”)
class CharbonnierLoss(nn.Module):
    def __init__(self, eps=1e-3):  # ë” ì•ˆì •ì ì¸ epsilon ê°’ìœ¼ë¡œ ì¡°ì •
        super(CharbonnierLoss, self).__init__()
        self.eps = eps

    def forward(self, pred, target):
        return torch.mean(torch.sqrt((pred - target) ** 2 + self.eps ** 2))


from basicsr.utils import get_root_logger

@MODEL_REGISTRY.register()
class ESRGANModel_V2(SRGANModel):
    """í’ì† ë°ì´í„° ì´ˆí•´ìƒí™” (SR) ìµœì í™” ESRGAN Model"""

    def __init__(self, opt):
        super(ESRGANModel_V2, self).__init__(opt)

        # âœ… ê¸°ì¡´ Charbonnier Loss (í”½ì…€ ê¸°ë°˜ ì†ì‹¤)
        self.cri_pix = CharbonnierLoss().to(self.device) if self.opt['train'].get('pixel_opt') else None

        # âœ… SSL ê¸°ë°˜ ìžê¸° ìœ ì‚¬ì„± ì†ì‹¤ ì¶”ê°€
        self.cri_ssl = patch_similarity_loss if self.opt['train'].get('ssl_opt') else None
        self.ssl_weight = self.opt['train'].get('ssl_weight', 0.05)

        # âœ… Wavelet ê¸°ë°˜ ê³ ì£¼íŒŒ ì†ì‹¤ ì¶”ê°€
        self.cri_wavelet = WaveletHighFrequencyLoss().to(self.device) if self.opt['train'].get('wavelet_opt') else None
        self.wavelet_weight = self.opt['train'].get('wavelet_weight', 0.05)

        # âœ… Logger ì¶”ê°€
        self.logger = get_root_logger()
        self.logger.info(f'Loss functions initialized - SSL: {self.ssl_weight}, Wavelet: {self.wavelet_weight}')

    def optimize_parameters(self, current_iter):
        # -------------------------------
        # âœ… ìƒì„±ê¸° ë„¤íŠ¸ì›Œí¬(net_g) ìµœì í™”
        # -------------------------------
        for p in self.net_d.parameters():
            p.requires_grad = False  # íŒë³„ê¸° ê³ ì •

        self.optimizer_g.zero_grad()
        self.output = self.net_g(self.lq)

        # âœ… ìƒì„±ê¸°ì˜ ì´ ì†ì‹¤ ì´ˆê¸°í™”
        l_g_total = torch.tensor(0.0, dtype=torch.float32, device=self.device)
        loss_dict = OrderedDict()

        if (current_iter % self.net_d_iters == 0 and current_iter > self.net_d_init_iters):
            # âœ… 1. Charbonnier Loss (í”½ì…€ ê¸°ë°˜ ì†ì‹¤)
            if self.cri_pix:
                l_g_pix = self.cri_pix(self.output, self.gt)
                l_g_total += l_g_pix
                loss_dict['l_g_pix'] = l_g_pix.detach()

            # âœ… 2. SSL(Self-Similarity Loss) ì ìš©
            if self.cri_ssl:
                l_g_ssl = self.cri_ssl(self.output, self.gt)
                weighted_ssl = self.ssl_weight * l_g_ssl
                l_g_total += weighted_ssl
                loss_dict['l_g_ssl'] = weighted_ssl.detach()

            # âœ… 3. Wavelet High-Frequency Loss ì ìš©
            if self.cri_wavelet:
                l_g_wavelet = self.cri_wavelet(self.output, self.gt)
                weighted_wavelet = self.wavelet_weight * l_g_wavelet
                l_g_total += weighted_wavelet
                loss_dict['l_g_wavelet'] = weighted_wavelet.detach()

            # âœ… 4. GAN Loss (Relativistic GAN)
            real_d_pred = self.net_d(self.gt).detach()
            fake_g_pred = self.net_d(self.output)

            l_g_real = self.cri_gan(real_d_pred - torch.mean(fake_g_pred), False, is_disc=False)
            l_g_fake = self.cri_gan(fake_g_pred - torch.mean(real_d_pred), True, is_disc=False)
            l_g_gan = (l_g_real + l_g_fake) / 2

            l_g_total += l_g_gan
            loss_dict['l_g_gan'] = l_g_gan.detach()

            # âœ… ìƒì„±ê¸° ì†ì‹¤ ì—­ì „íŒŒ ë° ì—…ë°ì´íŠ¸
            l_g_total.backward()
            self.optimizer_g.step()

        # -------------------------------
        # âœ… íŒë³„ê¸° ë„¤íŠ¸ì›Œí¬(net_d) ìµœì í™”
        # -------------------------------
        for p in self.net_d.parameters():
            p.requires_grad = True  # íŒë³„ê¸° í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ ë³€ê²½

        self.optimizer_d.zero_grad()

        fake_d_pred = self.net_d(self.output.detach())
        real_d_pred = self.net_d(self.gt)

        # âœ… íŒë³„ê¸° í•™ìŠµ ì•ˆì •ì„± ìœ ì§€
        l_d_real = self.cri_gan(real_d_pred - torch.mean(fake_d_pred.detach()), True, is_disc=True) * 0.5
        l_d_fake = self.cri_gan(fake_d_pred - torch.mean(real_d_pred.detach()), False, is_disc=True) * 0.5

        l_d_total = l_d_real + l_d_fake
        l_d_total.backward()
        self.optimizer_d.step()

        # âœ… Loss dict ì—…ë°ì´íŠ¸
        loss_dict['l_d_real'] = torch.clamp(l_d_real, min=1e-8).detach()
        loss_dict['l_d_fake'] = torch.clamp(l_d_fake, min=1e-8).detach()
        loss_dict['out_d_real'] = torch.mean(real_d_pred.detach())
        loss_dict['out_d_fake'] = torch.mean(fake_d_pred.detach())

        # âœ… `loss_dict`ì„ floatì´ ì—†ë„ë¡ ë³´ìž¥ í›„ ë³€í™˜
        for key, value in loss_dict.items():
            if isinstance(value, float):
                loss_dict[key] = torch.tensor(value, dtype=torch.float32, device=self.device).detach()

        self.log_dict = self.reduce_loss_dict(loss_dict)

        # -------------------------------
        # âœ… EMA ì—…ë°ì´íŠ¸ (Exponential Moving Average)
        # -------------------------------
        if self.ema_decay > 0:
            self.model_ema(decay=self.ema_decay)  # ðŸ”„ EMA ì ìš©


@MODEL_REGISTRY.register()
class ESRGANModel(SRGANModel):
    """ESRGAN ëª¨ë¸: ë‹¨ì¼ ì´ë¯¸ì§€ ì´ˆí•´ìƒë„(Single Image Super-Resolution)ë¥¼ ìœ„í•œ ëª¨ë¸."""

    def optimize_parameters(self, current_iter):
        # -------------------------------
        # ìƒì„±ê¸° ë„¤íŠ¸ì›Œí¬(net_g) ìµœì í™”
        # -------------------------------
        # íŒë³„ê¸° ë„¤íŠ¸ì›Œí¬(net_d)ì˜ ëª¨ë“  ë§¤ê°œë³€ìˆ˜ë¥¼ ê³ ì • (requires_grad=False)
        for p in self.net_d.parameters():
            p.requires_grad = False

        # ìƒì„±ê¸° ë„¤íŠ¸ì›Œí¬ì˜ ê¸°ìš¸ê¸° ì´ˆê¸°í™”
        self.optimizer_g.zero_grad()

        # ìž…ë ¥ ì €í•´ìƒë„ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•´ ìƒì„±ê¸° ì¶œë ¥ ê³„ì‚°
        self.output = self.net_g(self.lq)

        # ìƒì„±ê¸°ì˜ ì´ ì†ì‹¤ ì´ˆê¸°í™”
        l_g_total = 0
        loss_dict = OrderedDict()

        # ìƒì„±ê¸° í•™ìŠµ ì¡°ê±´: íŒë³„ê¸°ì˜ ì´ˆê¸°í™” ë‹¨ê³„ê°€ ëë‚¬ê³ , ì§€ì •ëœ ë°˜ë³µ ì£¼ê¸°ì— í•´ë‹¹í•  ê²½ìš°
        if (current_iter % self.net_d_iters == 0 and current_iter > self.net_d_init_iters):
            # 1. í”½ì…€ ì†ì‹¤ ê³„ì‚°
            if self.cri_pix:
                l_g_pix = self.cri_pix(self.output, self.gt)  # ìƒì„±ëœ ì´ë¯¸ì§€ì™€ ì‹¤ì œ ê³ í•´ìƒë„ ì´ë¯¸ì§€ ë¹„êµ
                l_g_total += l_g_pix  # ì´ ì†ì‹¤ì— ì¶”ê°€
                loss_dict['l_g_pix'] = l_g_pix  # ì†ì‹¤ ì •ë³´ ì €ìž¥

            # 2. Perceptual ì†ì‹¤ ë° ìŠ¤íƒ€ì¼ ì†ì‹¤ ê³„ì‚°
            if self.cri_perceptual:
                l_g_percep, l_g_style = self.cri_perceptual(self.output, self.gt)  # ìƒì„±ëœ ì´ë¯¸ì§€ì™€ ì‹¤ì œ ì´ë¯¸ì§€ ë¹„êµ
                if l_g_percep is not None:
                    l_g_total += l_g_percep  # Perceptual ì†ì‹¤ ì¶”ê°€
                    loss_dict['l_g_percep'] = l_g_percep
                if l_g_style is not None:
                    l_g_total += l_g_style  # ìŠ¤íƒ€ì¼ ì†ì‹¤ ì¶”ê°€
                    loss_dict['l_g_style'] = l_g_style

            # 3. GAN ì†ì‹¤ ê³„ì‚° (Relativistic GAN)
            real_d_pred = self.net_d(self.gt).detach()  # ì‹¤ì œ ì´ë¯¸ì§€ì— ëŒ€í•œ íŒë³„ê¸°ì˜ ì¶œë ¥
            fake_g_pred = self.net_d(self.output)  # ìƒì„±ëœ ì´ë¯¸ì§€ì— ëŒ€í•œ íŒë³„ê¸°ì˜ ì¶œë ¥

            # Relativistic GAN ì†ì‹¤ ê³„ì‚°
            l_g_real = self.cri_gan(real_d_pred - torch.mean(fake_g_pred), False, is_disc=False)
            l_g_fake = self.cri_gan(fake_g_pred - torch.mean(real_d_pred), True, is_disc=False)
            l_g_gan = (l_g_real + l_g_fake) / 2

            l_g_total += l_g_gan  # ì´ ì†ì‹¤ì— GAN ì†ì‹¤ ì¶”ê°€
            loss_dict['l_g_gan'] = l_g_gan

            # ì—­ì „íŒŒ ë° ìƒì„±ê¸° ìµœì í™”
            l_g_total.backward()
            self.optimizer_g.step()

        # -------------------------------
        # íŒë³„ê¸° ë„¤íŠ¸ì›Œí¬(net_d) ìµœì í™”
        # -------------------------------
        # íŒë³„ê¸°ì˜ ë§¤ê°œë³€ìˆ˜ ì—…ë°ì´íŠ¸ë¥¼ í—ˆìš© (requires_grad=True)
        for p in self.net_d.parameters():
            p.requires_grad = True

        # íŒë³„ê¸°ì˜ ê¸°ìš¸ê¸° ì´ˆê¸°í™”
        self.optimizer_d.zero_grad()

        # Relativistic GAN ì†ì‹¤ ê³„ì‚°
        # - ë¶„ì‚° í•™ìŠµ í™˜ê²½ì—ì„œ ë°œìƒí•  ìˆ˜ ìžˆëŠ” ì˜¤ë¥˜ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´
        #   ì‹¤ì œ(real)ì™€ ê°€ì§œ(fake)ì˜ ì—­ì „íŒŒë¥¼ ë¶„ë¦¬í•˜ì—¬ ì‹¤í–‰

        # 1. ì‹¤ì œ ì´ë¯¸ì§€ ì†ì‹¤ ê³„ì‚°
        fake_d_pred = self.net_d(self.output).detach()  # ìƒì„±ëœ ì´ë¯¸ì§€ì— ëŒ€í•œ íŒë³„ê¸° ì¶œë ¥ (ì—­ì „íŒŒ ì œì™¸)
        real_d_pred = self.net_d(self.gt)  # ì‹¤ì œ ì´ë¯¸ì§€ì— ëŒ€í•œ íŒë³„ê¸° ì¶œë ¥
        l_d_real = self.cri_gan(real_d_pred - torch.mean(fake_d_pred), True, is_disc=True) * 0.5
        l_d_real.backward()  # ì—­ì „íŒŒ ì‹¤í–‰

        # 2. ìƒì„±ëœ ì´ë¯¸ì§€ ì†ì‹¤ ê³„ì‚°
        fake_d_pred = self.net_d(self.output.detach())  # ìƒì„±ëœ ì´ë¯¸ì§€ ì¶œë ¥ (ì—­ì „íŒŒ ì œì™¸)
        l_d_fake = self.cri_gan(fake_d_pred - torch.mean(real_d_pred.detach()), False, is_disc=True) * 0.5
        l_d_fake.backward()  # ì—­ì „íŒŒ ì‹¤í–‰

        # íŒë³„ê¸° ìµœì í™”
        self.optimizer_d.step()

        # ì†ì‹¤ ê°’ ì €ìž¥
        loss_dict['l_d_real'] = l_d_real
        loss_dict['l_d_fake'] = l_d_fake
        loss_dict['out_d_real'] = torch.mean(real_d_pred.detach())  # ì‹¤ì œ ì´ë¯¸ì§€ì— ëŒ€í•œ í‰ê·  ì¶œë ¥ ì €ìž¥
        loss_dict['out_d_fake'] = torch.mean(fake_d_pred.detach())  # ìƒì„±ëœ ì´ë¯¸ì§€ì— ëŒ€í•œ í‰ê·  ì¶œë ¥ ì €ìž¥

        # ì†ì‹¤ ê°’ì„ ë¡œê·¸ì— ê¸°ë¡
        self.log_dict = self.reduce_loss_dict(loss_dict)

        # -------------------------------
        # EMA(Exponential Moving Average) ì—…ë°ì´íŠ¸
        # -------------------------------
        if self.ema_decay > 0:
            self.model_ema(decay=self.ema_decay)
